---
title: "Hands_on_III"
author: "Maria Pau Pijoan (mariapau.pijoan01@estudiant.upf.edu)"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"      
output:
  html_document:
    toc: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercise 1

**Build a Random Forest model to classify COVID-19 severity**

Using the training cohort provided in Supplementary Table 3, build a Random Forest classification model similar to the one in the publication to distinguish between non-Severe and Severe COVID-19 patients.

Report the following performance metrics from cross-validation:

-   Confusion matrix\
-   Accuracy\
-   ROC curve and AUC

How well does the Random Forest model separate non-Severe from Severe patients based on proteomic profiles?

```{r}

#PACKAGE INSTALLATION!:

# install.packages("readxl")
# install.packages("tidyverse")
# install.packages("janitor")
# install.packages("caTools")
# install.packages("randomForest")
# install.packages("tidymodels")
# install.packages("parsnip")
# install.packages("caret")
# install.packages("recipies")
# install.packages("pROC")

library(tidyverse)
library(readxl)
library(janitor) #clean_names function
library(dplyr)
library(caret)
library(recipes)
library(pROC)
library(rsample)


#-----DATA PREPARATION --------------

df <- read_excel("./1-s2.0-S0092867420306279-mmc3.xlsx", sheet = 2)
df_cleaned <- clean_names(df) 

#Now we have patients in columns and proteins in rows. To run most analyisis, the structure needs to be flipped. Columns (variable) --> something we measure (proteins) and in Rows (observations) --> individual entity (patients). 

#Extract actual patients IDs from the first row
new_headers <- df_cleaned[1, ] %>%
  select(patient_id:last_col()) %>% #start at patient_id til the enc
  unlist() %>%
  as.character()

#Rename the columns of the dataframe uing these real patient IDs
colnames(df_cleaned)[3:ncol(df_cleaned)] <- new_headers
#Columns 1 and 2 stay as they are, only columns 3 to onwards become XG1, XG2, etc

#Now transpose the table!

df_cleaned_final <- df_cleaned %>%
  slice(-1) %>%                         # Remove that first row now that we've used it for names
  pivot_longer(
    cols = all_of(new_headers),         # Pivot all the patient columns (XG1, XG2...)
    names_to = "patient_id",            # New column to hold the patient names
    values_to = "expression"            # New column to hold the numeric values
  ) %>%
  select(-gene_symbol) %>%
  pivot_wider(
    names_from = proteins_metabolites,          
    values_from = expression
  ) %>%
  # Convert the protein columns to numeric for the random forest
  mutate(across(-patient_id, as.numeric))


#Load the metadata to link patient_id with severity
df_metdata <- read_excel("./1-s2.0-S0092867420306279-mmc1.xlsx", sheet = 2)
df_metdata_cleaned <- clean_names(df_metdata) 
colnames(df_metdata_cleaned)[1] <- "patient_id" #changed the name from patient_id_a to patient_id to match the column names between the 2 dataframes

#Added in the df_final the group_d (severity) from df_metdata_cleaned
df_metdata_cleaned <- df_metdata_cleaned %>% select(patient_id, group_d)
df_final <- left_join(df_cleaned_final, df_metdata_cleaned, by = "patient_id") #2:non-Severe; 3: Severe
#As this data contains also non-COVID and healthy patients, and the exercise tells to filter only between non-severe and severe, I will filter the df_final to only have non-severe and severe patients (only 2 and 3).

df_final <- df_final %>%
  filter(group_d %in% c(2, 3)) %>%
  mutate(group_d = factor(group_d, levels = c(2,3), labels = c("non_Severe", "Severe"))) %>%
  select(-patient_id) #remove ID so it's not used as a predictor
table(df_final$group_d) #to see how many non-severe and severe patients are there and if we have removed the other groups correctly
#Now df_final contains all proteins in columns, patients in rows and in the last column it appears group_d (and only contians non-severe and severe patients)


#Now let's see the NA values (because RF cannot work if there are NA values)
sum(is.na(df_final)) #Total NA values

#Remove proteins with more than 20% missing values (this removes unreliable features). Because if a protein is missing in 80% of the patients, and I fill those gaps with the median of the 20% who actually had it, I am essentially inventing 80% of the data. And therefore, the RF might find a pattern in that protein, but that protein isn't biological, it's just a result of my imputation mehtod!!!
na_threshold <- 0.2
cols_to_keep <- colMeans(is.na(df_final)) <= na_threshold
df_final_0.2_removed <- df_final[ , cols_to_keep]
#At this point, we have removed some proteins that contained more than 20% missing values

df_final_cleaned <- df_final_0.2_removed %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))
sum(is.na(df_final_cleaned)) #now we have imputed the remaining NAs with the median

#clean the columns in the df_final_cleaned becasue RF algorithm got crazy
df_final_cleaned <- df_final_cleaned %>% 
  clean_names()

#At this point, I have obtained a tidy data format necessary for ML. As we have ~1600 proteins (features) but very few patients, RandomForest is a great choice because it handles "wide" data well. 

library(randomForest)

#Set seed for reproducibility (so I will get the same results every time)
rf_model <- randomForest(as.factor(group_d) ~ .,
                         data = df_final_cleaned,
                         importance = TRUE, #essential for exercise 2, where need to idenitfy the top 25 proteins. so it tells R to calculate which proteins were most helpful in splitting the severe from non-severe groups
                         ntree = 500)

print(rf_model)


# USING RECIPIE PACKAGE!!!!!

## recipie makes the workflow more professional, reproducible, and robust. Defines the data preprocessing steps as a "blueprint" and in exercise 3 when doing the test, I won't have to manually repeat the cleaning, just apply this blueprint. 



# Prepare data
# Ensure names are clean and variables are ready
df_final_preclean <- df_final %>% janitor::clean_names()

#--- COHORT SPLIT & INTERNAL DIVISION ------
set.seed(123)

#Create an 80/20 split stratified by the outcome (group_d)
data_split <- initial_split(df_final_preclean, prop = 0.8, strata = group_d)

#Development cohort (80%)
training_data <- training(data_split) #will have 24 observation, which is a 0.8% of the 31 ones initially

#Internal testing (20%)
internal_test_data <- testing(data_split)


#-----CROSS-VALIDATION (with the development cohort)-------

#Define the recipe (blueprint) (THIS IS DONE IN THE TEACHER SLIDES!!!!)
# We keep your recipe steps, but we'll pass this directly into the caret 'train' function
rf_recipe <- recipe(group_d ~ ., data = training_data) %>%
  step_nzv(all_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())

# set uo training controls
# This handles the 10-fold CV and collects data for the ROC curve
ctrl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",       # Necessary to plot the ROC later
  classProbs = TRUE,               # Necessary for AUC calculation
)

# Define tuning grid
# Testing different numbers of predictors at each split
p <- ncol(df_final_preclean) - 1 
mtry_grid <- data.frame(mtry = c(1, 2, floor(sqrt(p)), floor(p/3), 10, 30))

#------MODEL SELECTION & TRAINING-------

rf_model_2 <- train(
  rf_recipe, 
  data = training_data, 
  method = "rf",
  trControl = ctrl,
  tuneGrid = mtry_grid,
  metric = "Accuracy",                 
  ntree = 500,
  importance = TRUE
)

#------ RESULTS ANALYSIS------

# Performance during Cross-validation (the 80%)
print(rf_model_2)

# Performance on the Internal Hold-out (the 20%)
## This proves the model works on "unseen" data before going to the external cohort
internal_preds <- predict(rf_model_2, newdata = internal_test_data)
internal_cm <- confusionMatrix(internal_preds, internal_test_data$group_d)

print("----Internal 20% test results:-----")
print(internal_cm)


# Plot the Realistic ROC Curve
# Using the cross-validated predictions (prevents the "perfect square" look)
roc_obj <- roc(rf_model_2$pred$obs, rf_model_2$pred$Severe)

# Draw the plot (this creates 'plot.new' inside the file)
{
  plot(roc_obj, col = "blue", lwd = 3, main = "ROC Curve (10-Fold Cross-Validation)")

#Add the lines (these will now work)
abline(a = 0, b = 1, lty = 2, col = "grey")

# Add the legend
legend("bottomright", 
       legend = paste("AUC =", round(auc(roc_obj), 3)), 
       col = "blue", 
       lwd = 3, 
       bty = "n")

}




```

As shown in the updated results, the "manual" model (Random Forest with 500 trees) achieved an Out-of-Bag (OOB) error rate of 19.35%. In contrast, the recipe-based model (using the 80/20 split and cross-validation) showed a slightly higher error rate on the internal test set of 28.57% (equivalent to 71.43% accuracy).

The difference in performance is largely due to how each approach handles the data:

- Manual Model Performance: This model utilized all 31 samples (18 non-Severe, 13 Severe) to calculate its error estimate. By manually filtering proteins with >20% NAs, it likely retained a more concentrated set of high-quality predictors.

- Recipe-based Model Performance: This model was restricted to a smaller training set (80% of the cohort) to maintain strict architectural separation. Because it had fewer samples to learn from, its accuracy on the internal 20% hold-out was naturally lower at 71.43%.

- Feature Handling: The recipe model utilizes ```step_nzv()```, which is a conservative filter that removes variables with near-zero variance. This may have kept "noisy" proteins that the manual model's stricter NA threshold successfully eliminated.


To further validate the model, the ROC Curve and the Confusion Matrix were analyzed. The model achieves an outstanding AUC (Area Under the Curve) of 0.907. This indicates a roughly 91% probability that the model will correctly rank a randomly chosen Severe patient higher (in terms of severity risk) than a non-Severe patient.

- Cross-Validation: During the 10-fold cross-validation tuning process, the model reached a peak Accuracy of 81.67% (with mtry=40).

- Internal Testing: On a hold-out test set (the 20% internal division), the model maintained an Accuracy of 71.43%.

(mtry is a tunning parameter that controls the randomness and diversity of the trees in the forest. To decide which protein best separates Severe from non-Severe), it doesn't look at all 1639 proteins at once, instead the model randomly picks a subset of proteins, and the size of that subset is mtry, so the tree can only choose the best protein from that specific subset. So mtry = 30 means that the model had enough options to fins strong patterns but enough randomness to stay flexible, reaching a peak accuracy of 0.8167).

On the "unseen" 20% internal division, the model achieved an overall accuracy of 71,43%, correctly classifying 3 non_Severe and 2 Severe patients, misidentifying 1 non_Severe patient as Severe and 1 Severe patient as non_Severe one. Using "non_Severe" as the positive class, the model showed a sensitivity of 75% and a specificity of 66,67%. 

All in all, the proteomic signature is clearly a strong biological marker for disease severity. The model is highly sensitive, as shown by the sharp rise in the ROC curve. However, the drop from 81% accuracy in training to 71% in the internal test suggests some overfitting—likely due to the high number of predictors (1,638 proteins) compared to the small number of samples (24 patients in the training set). While the manual model shows a technically lower error rate (19.35%), the recipe-based model is the more robust choice for Exercise 3. It adheres to the Training Process Architecture by ensuring that the model's generalizability is tested on a truly "unseen" internal division before moving to the external cohort. The recipe model's AUC of 0.907 confirms that, despite the higher internal test error, it remains an excellent tool for separating non-Severe from Severe patients.

# Exercise 2

**Identify and interpret the most important features**

Using the trained Random Forest model from Exercise 1, identify the 25 most important protein features contributing to the classification. Discuss their biological relevance.

Compare your selected features to the proteins reported by the authors in Supplementary Table 5.

```{r}
library(tidyverse)
library(caret)
library(janitor)

# extract importance
importance_results <- varImp(rf_model_2, scale = FALSE)

# create top 25 list
top_25_list <- importance_results$importance %>%
  as.data.frame() %>%
  rownames_to_column(var = "Protein_ID") %>%
  # Fix the case: Ensure Protein_ID is Uppercase to match the metadata
  mutate(Protein_ID = toupper(Protein_ID)) %>%
  arrange(desc(Severe)) %>% 
  slice_head(n = 25)

#prepare comparison data
# We load the metadata and ensure the mapping ID is also Uppercase
df_table_5_meta <- read_excel("./1-s2.0-S0092867420306279-mmc5.xlsx", sheet = 2) %>% 
  clean_names() %>%
  slice(-1) %>% # Remove the row used for patient IDs
  select(proteins_metabolites, gene_symbol) %>%
  distinct() %>%
  mutate(proteins_metabolites = toupper(proteins_metabolites))

# merge
# Note: 'Protein_ID' from your list matches 'proteins_metabolites' from the metadata
top_25_named <- top_25_list %>% 
  left_join(df_table_5_meta, by = c("Protein_ID" = "proteins_metabolites"))

# view results
print(top_25_named)

# plot
plot(importance_results, top = 25, main = "Exercise 2: Top 25 Protein Predictors")

```

The gene symbols appearing next to your protein IDs (like SAA1, CRP, and HP) mean that the model successfully identified the same high-level biological markers that the researchers focused on. In the raw data, these are just technical accession numbers (like P02741), but by mapping them to gene symbols, it has been proven that the model "picked" the COVID-19 markers. If a protein has a gene symbol in the table, it is a known, named protein that the researchers also analyzed.

Based on the shared results obtained in this table, SAA1, SAA2 and CRP are the primary markers of the cytokine storm and systemic inflammation. SERPING1 and SERPINA3 indicate the activation of the body's innate defense and blood-clotting regulations. LUM and ITIH3 are linked to how the body repairs or scars connective tissue, particularly in the lungs. 



# Exercise 3

**External prediction on an independent test cohort**

Using the final Random Forest model trained on the full training cohort, predict disease severity for patients in the independent test cohort provided in Supplementary Table 4.

Your task is to:

-   Apply the trained model to the test cohort.  
-   Obtain predicted probabilities of Severe disease for each patient.  
-   Assign predicted class labels based on these probabilities.  
-   Compare predictions to the true clinical labels.  


How does the model perform on the independent test cohort?

```{r}

df_test_raw <- read_excel("./1-s2.0-S0092867420306279-mmc4.xlsx", sheet = 2) %>%
  clean_names()

#Comparison based on supplementary table 4
df_metdata_cleaned_2 <- df_metdata_cleaned %>%
  mutate(
    group = case_when(
      group_d %in% c(0, 1) ~ "Non-COVID-19",
      group_d == 2 ~ "Non-severe",
      group_d == 3 ~ "Severe",
      TRUE ~ NA_character_
    ),
    group = factor(group, levels = c("Non-severe", "Severe"))
  ) %>%
  select(patient_id, group) %>%
  filter(group %in% c("Non-severe", "Severe"))


new_headers <- df_test_raw[1, ] %>%
  select(patient_id:last_col()) %>% 
  unlist() %>%
  as.character()

colnames(df_test_raw)[3:ncol(df_test_raw)] <- new_headers

#pivot to tidy format
df_test_raw_final <- df_test_raw %>%
  slice(-1) %>%                     
  pivot_longer(
    cols = all_of(new_headers),         
    names_to = "patient_id",         
    values_to = "expression"           
  ) %>%
  select(-gene_symbol) %>%
  pivot_wider(
    names_from = proteins_metabolites,          
    values_from = expression
  ) %>%
  mutate(across(-patient_id, as.numeric))

df_test_with_labels <- df_test_raw_final %>%
  inner_join(df_metdata_cleaned_2, by = "patient_id") %>%
  mutate(group = recode(group, "Non-severe" = "non_Severe")) %>%
  mutate(group = factor(group, levels = c("non_Severe", "Severe"))) %>%
  clean_names()



model_predictors <- rf_model_2$finalModel$xNames
# model_predictors

missing_features <- setdiff(model_predictors, colnames(df_test_with_labels))
# missing_features

df_test_with_labels[missing_features] <- NA #Add the missing columns to the test set, filled with NAs
#This is done because RF requires the exact same column names in the exact order as the training data.

x_test <- df_test_with_labels %>%
  dplyr::select(all_of(model_predictors))


#Predict disease severity
#use the rf_model_2 to see if it can correctly guess which proteins are severe

# predictions (make sure these are vectors, not data frames) --> Apply trained model to the test cohort
test_predictions <- predict(rf_model_2, newdata = x_test)
test_probabilities <- predict(rf_model_2, newdata = x_test, type = "prob")

# If predict() returned a data frame (tidymodels), extract the column:
if (is.data.frame(test_predictions)) test_predictions <- test_predictions$.pred

# Probabilities column name can differ: "Severe" (caret) vs ".pred_Severe" (tidymodels)
prob_severe <- if (is.data.frame(test_probabilities)) {
  if ("Severe" %in% colnames(test_probabilities)) test_probabilities$Severe else test_probabilities$.pred_Severe
} else {
  test_probabilities[, "Severe"]
}


# Results table
test_results_table <- tibble(
  patient_id = df_test_with_labels$patient_id,
  Actual = df_test_with_labels$group,
  Predicted_Class = test_predictions,
  Prob_Severe = prob_severe
) %>%
  mutate(Correct = ifelse(Actual == Predicted_Class, "Correct", "Wrong"))

print(test_results_table)

# Confusion matrix (predicted vs truth)
final_metrics <- confusionMatrix(
  factor(test_results_table$Predicted_Class, levels = levels(test_results_table$Actual)),
  test_results_table$Actual,
  positive = "Severe"
)

print(final_metrics)

```
The final Random Forest model trained on the full training cohort demonstrated strong performance on the independent test cohort (n=10). Because it was used the ```recipies``` framework, the test data was automatically preprocessed (imputed and normalized) using the parameters established during the training phase. The model achieved an accuracy of 90%, correctly classifying 9 out of 10 patients. Importantly, all Severe patients were correctly identified (Sensitivity = 1.00), while one non-Severe patient was misclassified (Specificity = 0.83). The balanced accuracy was 0.92 and Cohen’s Kappa was 0.80, indicating strong agreement between the model's predictions and the actual clinical labels. These results suggest that the proteomic signature generalizes well to independent patients and maintains strong discriminatory ability. The model's perfromance on the independent cohort (90% accuracy) is notably higher than its performance on the internal 20% hold-out (71,43%). This suggests that the proteomic signature is highly robust biological marker for severity that remains consistent across different patient groups. Although performance is strong, the small size of the independent test cohort results in wide confidence intervals and warrants validation in larger cohorts.




# session info {.unnumbered}

```{r, results='asis',  echo=FALSE, message=FALSE }
sessionInfo()
```
